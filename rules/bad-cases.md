# Bad Cases 记录

> 执行 /madness 时对照此文件自检，避免重蹈覆辙。

---

## Bad Case #1: 2026-02-11 init 基线报告

**项目**: 访谈报告优化（31 个会话，302+ 条用户消息）

### 发生了什么

执行 /madness init 时，生成了一份"概要版"基线报告就直接存盘了。用户反馈："太简洁了，远远不够"、"没带我成长"、"改进要怎么改进"。被迫重做深度版。

### 具体问题

#### 问题 1: 报告只有统计没有分析

**错误输出**:
```
### 摩擦类型排名
| rework_from_poor_planning | 12 | 21% | 规划不足导致返工 |
```

**用户反馈**: "远远不够"

**正确输出应该是**:
```
### 根因 1: 规划不足导致返工（12次）
这不是"少规划"的问题，是"规划的东西不对"的问题。
用户并非没有规划——Feb 3 开始就使用 planning-with-files。
问题是规划只定义了"做什么"，没有定义：
1. 分类标准（"什么算任务型，什么算IP型"）
2. 输出格式（"表格长什么样"）
...
具体改进 — Phase 0 必须完成的 4 件事：[检查清单]
```

**教训**: 统计数据只是入口，必须追到"为什么"和"怎么改"。

---

#### 问题 2: 改进建议是空话

**错误输出**:
```
### 仍需改进
1. Prompt 做减法不够彻底
2. Phase 间 checkpoint 缺失
3. 并行模式未标准化
```

**用户反馈**: "改进要怎么改进？并行任务的最佳实践是什么？减法不够彻底，怎么做才彻底？"

**正确输出应该是**:
```
### Prompt 做减法的具体方法
原则: 写步骤（procedural）不写原则（declarative）

❌ 坏 Prompt（80行）:
"你需要深入分析用户需求，关注代际差异..."

✅ 好 Prompt（30行）:
"对每个用户执行以下步骤:
1. 找到签到任务相关段落
2. 摘录用户原话（不改写）
3. 按以下标准分类态度: [见分类表]"

Prompt 金字塔: 顶层≤50行核心规则 → 中层≤150行必做步骤 → 底层按需
A/B 测试: 新版 Prompt 先在 1-2 份文件上测试，对比旧版差异，确认后再推广
```

**教训**: 每条改进必须回答"第一步做什么、第二步做什么、怎么判断做对了"。

---

#### 问题 3: 没有确认就存盘

**错误行为**: 生成报告后直接 `Write` 到 `.retro/reviews/` 并更新 `state.json`。

**用户反馈**: "没有按照我的要求给我确认"

**正确行为**:
1. 先展示报告全文
2. 询问"需要补充或调整吗？"
3. 用户说"OK"后才写入文件

**教训**: SKILL.md 写了"确认后存入"但执行时跳过了。必须在代码流程中设置硬性 checkpoint。

---

#### 问题 4: 一次成型而非迭代

**错误行为**: 子智能体提取 facet → 聚合统计 → 直接生成报告。只做了一轮分析。

**正确行为**: 两阶段分析：
- 阶段 A: 结构化提取（统计+模式识别）
- 阶段 B: 深度归因（回到原始数据追溯用户原话、分析根因、设计 SOP）

**教训**: 复盘报告的价值在深度，不在速度。宁可多花一轮子智能体，不要输出浅层概括。

---

#### 问题 5: 子智能体 JSON 输出出错

**错误行为**: 子智能体在 `loop_detail` 字段中写了 `"差异化"` 这样的中文引号（用 ASCII `"` 包裹），导致 JSON 解析失败。

**根因**: Prompt 中没有约束 JSON 输出格式。

**正确做法**: 在子智能体 Prompt 中加入：
```
输出要求：
1. 必须是有效的 JSON 数组
2. 字符串值中不要使用未转义的 ASCII 双引号，中文引号用「」替代
3. 不要在 JSON 中包含注释
```

**教训**: 子智能体的输出格式必须在 Prompt 中显式约束，不能假设它会自动生成有效 JSON。

---

---

#### 问题 6: 低质量 Facet 未被发现

**错误行为**: 子智能体对一个 30+ 消息的复杂 session 提取的 facet 中，`goal` 只写了"调试"，`friction` 为空，`ai_collab` 全部为空字符串。实际 session 中有明显的 AI 依赖和逻辑跳跃，但 facet 完全没体现。由于缺少 `extraction_confidence` 字段，这个低质量 facet 混入了聚合分析，导致最终报告漏掉了关键问题。

**正确做法**:
1. 子智能体必须为每个 facet 输出 `extraction_confidence`（0-1），反映对自身提取质量的信心
2. 当 facet 数量 >5 时，执行 Step 3b 抽检：随机抽 2 个展示给用户确认
3. `extraction_confidence < 0.5` 的 facet 应在聚合分析前被标记，优先复核

**教训**: Facet 质量是分析链的根基。没有质量检验的 facet 提取等于在沙地上建大楼。

---

---

## Bad Case #7: 2026-02-24 init 苏格拉底归因方向性错误

**项目**: SQ_Help（帮助页视频，7 个会话）

### 发生了什么

执行 /madness init 后，苏格拉底质询将多次返工归因为用户行为（scope_creep、lazy_prompting），用户强力反驳后 AI 才修正诊断——实际根因是 AI 执行不忠实（参数偏差、未遵循 CLAUDE.md 规范、首轮方向性错误）。

### 具体问题

#### 问题 1: 归因方向全面偏向用户

**错误质询**:
```
「你在会话 X 中让 AI 做了 Y，但结果返工了 8 轮。
  这是不是 scope_creep？你是否在过程中不断追加需求？」
```

**用户反驳**:
```
「不是我 scope_creep，是 AI 第一轮实现就跑偏了。
  我给了 Playground 参数，AI 没有按原值执行。
  我给了参考代码，AI 理解方向就是错的。这是 AI 的问题，不是我的。」
```

**根因**: socratic.md 的证据构建步骤只检测 5 类**用户行为**（sycophancy、logic_leap、lazy_prompting、automation_surrender、anchoring_effect），没有任何维度检测 **AI 执行质量**（参数忠实度、规范遵循度、首轮方向正确率）。

#### 问题 2: 协议级缺陷而非模型偶发

这不是模型"偶尔犯错"——是协议本身就引导质询方向偏向"用户做错了什么"。5 类检测**全部面向用户**，即使模型再强，没有 AI 执行审计维度就必然遗漏 AI 自身的问题。

### 修复

1. 新增 `ai_execution` facet 字段（param_fidelity、spec_compliance、first_round_accuracy、rework_attribution）
2. socratic.md 新增前置步骤 0（AI 执行质量自审），归因优先级：AI > 流程 > 用户
3. 所有分析组（init/mid/final）新增 AI 执行审计维度

### 教训

**归因优先级必须是：AI 执行 → 流程/工具 → 用户行为。** 先审查 AI 自身是否忠实执行，排除 AI 因素后再质疑用户行为。否则每次 Socratic 质询都可能方向性错误，需要用户强力反驳才能纠偏——而这恰恰是 Socratic 质询想要避免的"用户不反思"问题的反面。

---

### 总结：7 条自检规则

执行 /madness 时，在输出报告前对照：

1. **每个摩擦点有用户原话证据吗？** → 没有就回阶段 B 补
2. **每条改进有步骤+检查点吗？** → 没有就补上"第一步做X、第二步做Y、判断标准Z"
3. **先展示再确认了吗？** → 没有就不要写文件
4. **做了两阶段分析吗？** → 只做了统计就不要输出报告
5. **子智能体 Prompt 约束了 JSON 格式吗？** → 没有就加上格式约束
6. **Facet 质量有保障吗？** → >5 个 facet 时是否执行了 Step 3b 抽检？有无 extraction_confidence < 0.5 的 facet 被漏检？
7. **Socratic 归因方向正确吗？** → 先检查 ai_execution 有无问题，有则优先质询 AI 执行偏差，再质询用户行为
